# vk_cloud_peft_gpt

В данном репозитории мы демонстрируем fine-tuning больших моделей с помощью PEFT и LoRa.  
Подробнее можно ознакомиться по ссылке https://huggingface.co/blog/peft  
  
В качестве базовой модели используется Pythia-12b  
https://huggingface.co/EleutherAI/pythia-12b  
  
Репозиторий состоит из двух Jupyter тетрадок.  
Первая отвечает за тренировку модели.   
Вторая - сервинг и тест полученной модели.  
  
Репозиторий подготовлен к выступлению на VK Cloud Conf 2023  
https://mcs.mail.ru/events/vk-cloud-conf23   
