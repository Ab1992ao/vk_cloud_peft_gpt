# vk_cloud_peft_gpt

В данном репозитории мы демонстрируем fine-tuning больших моделей с помощью PEFT и LoRa.  
Подробнее можно ознакомиться по ссылке https://huggingface.co/blog/peft  
  
В качестве базовой модели используется Pythia-12b  
https://huggingface.co/EleutherAI/pythia-12b  

Тренировка и тестирование проводилось с использованием Tesla A100 40 GB в [VK Cloud](https://mcs.mail.ru/docs/ml/mlplatform)   
  
Репозиторий состоит из двух Jupyter тетрадок.  
Первая отвечает за тренировку модели.   
Вторая - сервинг и тест полученной модели.  
  
Репозиторий подготовлен к выступлению на VK Cloud Conf 2023  
https://mcs.mail.ru/events/vk-cloud-conf23     

Вопросы можно задать в telegram https://t.me/volinski  
Также подписывайтесь на наш канал https://t.me/sterodata  
